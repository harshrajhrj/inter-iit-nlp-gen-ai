{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311cdc57",
   "metadata": {},
   "source": [
    "##### Input Embeddings and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aede67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abb9f0",
   "metadata": {},
   "source": [
    "1. token embedding player\n",
    "2. constant positional encoding matrix (sinusoidal pattern)\n",
    "3. return token + positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa182238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len=512):\n",
    "        super().__init__()\n",
    "        # token embedding layer: we are mapping token indices to embedding vectors\n",
    "        self.token_embedding=nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dim=embed_dim\n",
    "\n",
    "        # creating constant positional encoding matrix with sinusoidal pattern\n",
    "        pe=torch.zeros(max_len, embed_dim)\n",
    "        position=torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # shape: (max_len, 1)\n",
    "\n",
    "        # computing the div_term for the sinusoidal frequencies\n",
    "        div_term=torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        # applying sine to even indices in the embedding dimension\n",
    "        pe[:, 0::2]=torch.sin(position * div_term)\n",
    "\n",
    "        # applying cosine to odd indices in the embedding dimension\n",
    "        pe[:, 1::2]=torch.cos(position * div_term)\n",
    "\n",
    "        # register pe as a buffer so it's saved with the model but not as a parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # \"x\" is tensor of shape (batch_size, seq_len) with token indices\n",
    "        seq_len=x.size(1)\n",
    "\n",
    "        # get token embeddings: (batch_size, seq_len, embed_dim)\n",
    "        token_emb=self.token_embedding(x)\n",
    "\n",
    "        # get positional embeddings for the sequence length: (1, seq_len, embed_dim)\n",
    "        pos_emb=self.pe[:seq_len, :].unsqueeze(0)\n",
    "\n",
    "        # add token and positional embeddings\n",
    "        return token_emb + pos_emb # tensor of shape (batch_size, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ec6fd",
   "metadata": {},
   "source": [
    "##### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, visualize=False):\n",
    "    d_k=query.size(-1)  # get the dimension of the key (embedding size)\n",
    "\n",
    "    # computing raw attention scores by matrix multiplying query and key_transpose, then scale\n",
    "    scores=torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # if a mask is provided, set masked positions to a large negative value\n",
    "    if mask is not None:\n",
    "        scores=scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # applying softmax to get normalized attention weights\n",
    "    attn_weights=torch.softmax(scores, dim=-1)\n",
    "\n",
    "    # multiplying attention weights by the value vectors to get the output\n",
    "    op=torch.matmul(attn_weights, value)\n",
    "\n",
    "    # if visualize is True, return both output and attention weights\n",
    "    if visualize:\n",
    "        return op, attn_weights\n",
    "    # otherwise, return only the output\n",
    "    return op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d993f0",
   "metadata": {},
   "source": [
    "##### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim=embed_dim\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=embed_dim // num_heads\n",
    "\n",
    "        # linear layers to project input to queries, keys, and values\n",
    "        self.q_proj=nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj=nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj=nn.Linear(embed_dim, embed_dim)\n",
    "        # final linear layer to combine outputs from all heads\n",
    "        self.out_proj=nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, visualize=False):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "\n",
    "        # auxiliary function to reshape input for multi-head attention\n",
    "        def shape(x):\n",
    "            # (batch_size, seq_len, embed_dim) -> (batch_size, num_heads, seq_len, head_dim)\n",
    "            return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Project inputs to multi-head Q, K, V\n",
    "        q=shape(self.q_proj(query))\n",
    "        k=shape(self.k_proj(key))\n",
    "        v=shape(self.v_proj(value))\n",
    "\n",
    "        # if mask is provided, expand its dimensions for all heads\n",
    "        if mask is not None:\n",
    "            mask=mask.unsqueeze(1)  # (batch_size, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "        # compute attention output (and optionally attention weights) for all heads\n",
    "        attn_output=scaled_dot_product_attention(q, k, v, mask=mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            attn_output, attn_weights = attn_output\n",
    "\n",
    "        # concatenate outputs from all heads\n",
    "        attn_output=attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # final linear projection\n",
    "        output=self.out_proj(attn_output)\n",
    "\n",
    "        # return output (and attention weights if visualize=True)\n",
    "        if visualize:\n",
    "            return output, attn_weights\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32949a5",
   "metadata": {},
   "source": [
    "##### Feed-Forward Networks and Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm=nn.LayerNorm(embed_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer_out):\n",
    "        # residual connection followed by layer normalization\n",
    "        return self.norm(x + self.dropout(sublayer_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4852338",
   "metadata": {},
   "source": [
    "##### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # multi-head self-attention mechanism\n",
    "        self.self_attn=MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "        # add & Norm layer after self-attention\n",
    "        self.addnorm1=AddNorm(embed_dim, dropout)\n",
    "\n",
    "        # position-wise Feed-Forward Network\n",
    "        self.ffn=PositionwiseFeedForward(embed_dim, ffn_dim, dropout)\n",
    "\n",
    "        # add & Norm layer after feed-forward\n",
    "        self.addnorm2=AddNorm(embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x, mask=None, visualize=False):\n",
    "        # self-attention sublayer with residual connection and normalization\n",
    "        # attn_out: output of self-attention; attn_weights: attention weights (if visualize=True)\n",
    "        attn_out=self.self_attn(x, x, x, mask=mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            attn_out, attn_weights = attn_out  # unpack output and attention weights\n",
    "\n",
    "        # add & Norm: residual connection (x + attn_out) followed by layer normalization\n",
    "        x=self.addnorm1(x, attn_out)\n",
    "\n",
    "        # feed-forward sublayer with residual connection and normalization\n",
    "        ffn_out=self.ffn(x)\n",
    "\n",
    "        # add & Norm: residual connection (x + ffn_out) followed by layer normalization\n",
    "        x=self.addnorm2(x, ffn_out)\n",
    "        if visualize:\n",
    "            return x, attn_weights  # return output and attention weights for visualization\n",
    "        return x  # return output only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85879898",
   "metadata": {},
   "source": [
    "##### Decoder Layer with Masked Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # masked multi-head self-attention (causal): attends to previous tokens only\n",
    "        self.self_attn=MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.addnorm1=AddNorm(embed_dim, dropout)\n",
    "\n",
    "        # encoder-decoder (cross) attention: attends to encoder outputs\n",
    "        self.cross_attn=MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.addnorm2=AddNorm(embed_dim, dropout)\n",
    "\n",
    "        # position-wise feed-forward network\n",
    "        self.ffn=PositionwiseFeedForward(embed_dim, ffn_dim, dropout)\n",
    "        self.addnorm3=AddNorm(embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None, visualize=False):\n",
    "        attn_weights={}\n",
    "\n",
    "        # we are numbering the steps for clarity\n",
    "        # 1. masked self-attention: each position can only attend to earlier positions (causal)\n",
    "        self_attn_out=self.self_attn(x, x, x, mask=tgt_mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            self_attn_out, self_attn_weights=self_attn_out  # unpack output and attention weights\n",
    "            attn_weights['self_attn']=self_attn_weights\n",
    "\n",
    "        # add & Norm: residual connection and layer normalization\n",
    "        x=self.addnorm1(x, self_attn_out)\n",
    "\n",
    "        # 2. cross-attention: decoder attends to encoder outputs (full attention)\n",
    "        cross_attn_out=self.cross_attn(x, enc_out, enc_out, mask=memory_mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            cross_attn_out, cross_attn_weights=cross_attn_out  # unpack output and attention weights\n",
    "            attn_weights['cross_attn']=cross_attn_weights\n",
    "\n",
    "        # add & Norm: residual connection and layer normalization\n",
    "        x=self.addnorm2(x, cross_attn_out)\n",
    "\n",
    "        # 3. feed-forward network: position-wise transformation\n",
    "        ffn_out=self.ffn(x)\n",
    "\n",
    "        # add & Norm: residual connection and layer normalization\n",
    "        x=self.addnorm3(x, ffn_out)\n",
    "\n",
    "        # if visualize is True, return attention weights for analysis\n",
    "        if visualize:\n",
    "            return x, attn_weights\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07126fb7",
   "metadata": {},
   "source": [
    "##### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ffn_dim, num_encoder_layers, num_decoder_layers, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # embedding layers for source and target sequences (with positional encoding)\n",
    "        self.src_embedding=TokenPositionalEmbedding(vocab_size, embed_dim, max_len)\n",
    "        self.target_embedding=TokenPositionalEmbedding(vocab_size, embed_dim, max_len)\n",
    "\n",
    "        # stack of encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, ffn_dim, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # stack of decoder layers\n",
    "        self.decoder_layers=nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, ffn_dim, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # final linear layer to project decoder output to vocabulary logits\n",
    "        self.output_proj=nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # initialize parameters (weights)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # xavier uniform initialization for all weights with more than 1 dimension\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, target, src_mask=None, target_mask=None, memory_mask=None, visualize=False):\n",
    "        attn_weights = [] if visualize else None\n",
    "\n",
    "        # 1. embed source and target tokens (add positional encoding)\n",
    "        src_emb=self.src_embedding(src)  # (batch_size, src_seq_len, embed_dim)\n",
    "        target_emb=self.target_embedding(target)  # (batch_size, target_seq_len, embed_dim)\n",
    "\n",
    "        # 2. pass source embeddings through encoder stack\n",
    "        enc_out=src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            if visualize:\n",
    "                # Get encoder output and attention weights for visualization\n",
    "                enc_out, enc_attn=layer(enc_out, mask=src_mask, visualize=True)\n",
    "                attn_weights.append({'encoder': enc_attn})\n",
    "            else:\n",
    "                enc_out=layer(enc_out, mask=src_mask)\n",
    "\n",
    "        # 3. pass target embeddings and encoder output through decoder stack\n",
    "        dec_out=target_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            if visualize:\n",
    "                # get decoder output and attention weights for visualization\n",
    "                dec_out, dec_attn=layer(dec_out, enc_out, target_mask=target_mask, memory_mask=memory_mask, visualize=True)\n",
    "                attn_weights.append({'decoder': dec_attn})\n",
    "            else:\n",
    "                dec_out=layer(dec_out, enc_out, target_mask=target_mask, memory_mask=memory_mask)\n",
    "\n",
    "        # 4. project decoder output to vocabulary logits for each position\n",
    "        logits=self.output_proj(dec_out)  # (batch_size, target_seq_len, vocab_size)\n",
    "\n",
    "        # return logits and optionally attention weights for visualization\n",
    "        if visualize:\n",
    "            return logits, attn_weights\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model hyperparameters\n",
    "vocab_size = 10000\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "ffn_dim = 2048\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "max_len = 512\n",
    "dropout = 0.1\n",
    "\n",
    "# instantiate the Transformer model\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ffn_dim=ffn_dim,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    max_len=max_len,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# move model to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
