{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311cdc57",
   "metadata": {},
   "source": [
    "##### Input Embeddings and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aede67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abb9f0",
   "metadata": {},
   "source": [
    "##### Input Embeddings and Positional Encoding\n",
    "1. token embedding player\n",
    "2. constant positional encoding matrix (sinusoidal pattern)\n",
    "3. return token + positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa182238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, sizeOfVocab, embedDimension, maxLength=512):\n",
    "        super().__init__()\n",
    "        # token embedding layer-> we are mapping token indices to embedding vectors\n",
    "        self.tokenEmbedding=nn.Embedding(sizeOfVocab, embedDimension)\n",
    "        self.embedDimension=embedDimension\n",
    "\n",
    "        # creating constant positional encoding matrix with sinusoidal pattern\n",
    "        pe=torch.zeros(maxLength, embedDimension)\n",
    "        position=torch.arange(0, maxLength, dtype=torch.float).unsqueeze(1)  # shape-> (maxLength, 1)\n",
    "\n",
    "        # computing the divisionTerm for the sinusoidal frequencies\n",
    "        divisionTerm=torch.exp(torch.arange(0, embedDimension, 2).float() * (-math.log(10000.0) / embedDimension))\n",
    "        \n",
    "        # applying sine to even indices in the embedding dimension\n",
    "        pe[:, 0::2]=torch.sin(position * divisionTerm)\n",
    "\n",
    "        # applying cosine to odd indices in the embedding dimension\n",
    "        pe[:, 1::2]=torch.cos(position * divisionTerm)\n",
    "\n",
    "        # register pe as a buffer so it's saved with the model but not as a parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # \"x\" is tensor of shape (batch_size,sequenceLength) with token indices\n",
    "        sequenceLength=x.size(1)\n",
    "\n",
    "        # get token embeddings-> (batch_size,sequenceLength,embedDimension)\n",
    "        token_emb=self.tokenEmbedding(x)\n",
    "\n",
    "        # get positional embeddings for the sequence length-> (1,sequenceLength,embedDimension)\n",
    "        pos_emb=self.pe[:sequenceLength, :].unsqueeze(0)\n",
    "\n",
    "        # add token and positional embeddings\n",
    "        return token_emb + pos_emb # tensor of shape (batch_size,sequenceLength,embedDimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ec6fd",
   "metadata": {},
   "source": [
    "##### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, visualize=False):\n",
    "    d_k=query.size(-1)  # get the dimension of the key (embedding size)\n",
    "\n",
    "    # computing raw attention scores by matrix multiplying query and key_transpose, then scale\n",
    "    scores=torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # if a mask is provided,set masked positions to a large negative value\n",
    "    if mask is not None:\n",
    "        scores=scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # applying softmax to get normalized attention weights\n",
    "    attentionWeights=torch.softmax(scores, dim=-1)\n",
    "\n",
    "    # multiplying attention weights by the value vectors to get the output\n",
    "    op=torch.matmul(attentionWeights, value)\n",
    "\n",
    "    # if visualize is True, return both output and attention weights\n",
    "    if visualize:\n",
    "        return op, attentionWeights\n",
    "    # otherwise, return only the output\n",
    "    return op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d993f0",
   "metadata": {},
   "source": [
    "##### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedDimension, numHeads):\n",
    "        super().__init__()\n",
    "        assert embedDimension % numHeads == 0, \"embedDimension must be divisible by numHeads\"\n",
    "        self.embedDimension=embedDimension\n",
    "        self.numHeads=numHeads\n",
    "        self.head_dim=embedDimension // numHeads\n",
    "\n",
    "        # linear layers to project input to queries, keys, and values\n",
    "        self.q_proj=nn.Linear(embedDimension, embedDimension)\n",
    "        self.k_proj=nn.Linear(embedDimension, embedDimension)\n",
    "        self.v_proj=nn.Linear(embedDimension, embedDimension)\n",
    "        # final linear layer to combine outputs from all heads\n",
    "        self.out_proj=nn.Linear(embedDimension, embedDimension)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, visualize=False):\n",
    "        batch_size, sequenceLength, _ = query.size()\n",
    "\n",
    "        # auxiliary function to reshape input for multi-head attention\n",
    "        def shape(x):\n",
    "            # (batch_size, sequenceLength, embedDimension) -> (batch_size, numHeads, sequenceLength, head_dim)\n",
    "            return x.view(batch_size, sequenceLength, self.numHeads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Project inputs to multi-head Q, K, V\n",
    "        q=shape(self.q_proj(query))\n",
    "        k=shape(self.k_proj(key))\n",
    "        v=shape(self.v_proj(value))\n",
    "\n",
    "        # if mask is provided, expand its dimensions for all heads\n",
    "        if mask is not None:\n",
    "            mask=mask.unsqueeze(1)  # (batch_size, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "        # compute attention output (and optionally attention weights) for all heads\n",
    "        attn_output=scaled_dot_product_attention(q, k, v, mask=mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            attn_output, attentionWeights = attn_output\n",
    "\n",
    "        # concatenate outputs from all heads\n",
    "        attn_output=attn_output.transpose(1, 2).contiguous().view(batch_size, sequenceLength, self.embedDimension)\n",
    "        \n",
    "        # final linear projection\n",
    "        output=self.out_proj(attn_output)\n",
    "\n",
    "        # return output (and attention weights if visualize=True)\n",
    "        if visualize:\n",
    "            return output, attentionWeights\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32949a5",
   "metadata": {},
   "source": [
    "##### Feed-Forward Networks and Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, embedDimension, feedForwardDimension, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(embedDimension, feedForwardDimension)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(feedForwardDimension, embedDimension)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequenceLength, embedDimension)\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, embedDimension, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm=nn.LayerNorm(embedDimension)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer_out):\n",
    "        # residual connection followed by layer normalization\n",
    "        return self.norm(x + self.dropout(sublayer_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4852338",
   "metadata": {},
   "source": [
    "##### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedDimension, numHeads, feedForwardDimension, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # multi-head self-attention mechanism\n",
    "        self.self_attn=MultiHeadAttention(embedDimension, numHeads)\n",
    "\n",
    "        # add & Norm layer after self-attention\n",
    "        self.addnorm1=AddNorm(embedDimension, dropout)\n",
    "\n",
    "        # position-wise Feed-Forward Network\n",
    "        self.ffn=PositionwiseFeedForward(embedDimension, feedForwardDimension, dropout)\n",
    "\n",
    "        # add & Norm layer after feed-forward\n",
    "        self.addnorm2=AddNorm(embedDimension, dropout)\n",
    "\n",
    "    def forward(self, x, mask=None, visualize=False):\n",
    "        # self-attention sublayer with residual connection and normalization\n",
    "        # attn_out-> output of self-attention; attentionWeights-> attention weights (if visualize=True)\n",
    "        attn_out=self.selfAttention(x, x, x, mask=mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            attn_out, attentionWeights = attn_out  # unpack output and attention weights\n",
    "\n",
    "        # add & Norm-> residual connection (x + attn_out) followed by layer normalization\n",
    "        x=self.addnorm1(x, attn_out)\n",
    "\n",
    "        # feed-forward sublayer with residual connection and normalization\n",
    "        ffn_out=self.ffn(x)\n",
    "\n",
    "        # add & Norm-> residual connection (x + ffn_out) followed by layer normalization\n",
    "        x=self.addnorm2(x, ffn_out)\n",
    "        if visualize:\n",
    "            return x, attentionWeights  # return output and attention weights for visualization\n",
    "        return x  # return output only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85879898",
   "metadata": {},
   "source": [
    "##### Decoder Layer with Masked Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedDimension, numHeads, feedForwardDimension, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # masked multi-head self-attention (causal)-> attends to previous tokens only\n",
    "        self.selfAttention=MultiHeadAttention(embedDimension, numHeads)\n",
    "        self.addnorm1=AddNorm(embedDimension, dropout)\n",
    "\n",
    "        # encoder-decoder (cross) attention-> attends to encoder outputs\n",
    "        self.crossAttention=MultiHeadAttention(embedDimension, numHeads)\n",
    "        self.addnorm2=AddNorm(embedDimension, dropout)\n",
    "\n",
    "        # position-wise feed-forward network\n",
    "        self.ffn=PositionwiseFeedForward(embedDimension, feedForwardDimension, dropout)\n",
    "        self.addnorm3=AddNorm(embedDimension, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, tgt_mask=None, memoryMask=None, visualize=False):\n",
    "        attentionWeights={}\n",
    "\n",
    "        # we are numbering the steps for clarity\n",
    "        # 1. masked self-attention-> each position can only attend to earlier positions (causal)\n",
    "        selfAttentionOutput=self.selfAttention(x, x, x, mask=tgt_mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            selfAttentionOutput, selfAttentionWeights=selfAttentionOutput  # unpack output and attention weights\n",
    "            attentionWeights['self_attn']=selfAttentionWeights\n",
    "\n",
    "        # add & Norm-> residual connection and layer normalization\n",
    "        x=self.addnorm1(x, selfAttentionOutput)\n",
    "\n",
    "        # 2. cross-attention-> decoder attends to encoder outputs (full attention)\n",
    "        crossAttentionOutput=self.crossAttention(x, enc_out, enc_out, mask=memoryMask, visualize=visualize)\n",
    "        if visualize:\n",
    "            crossAttentionOutput, crossAttentionWeights=crossAttentionOutput  # unpack output and attention weights\n",
    "            attentionWeights['cross_attn']=crossAttentionWeights\n",
    "\n",
    "        # add & Norm-> residual connection and layer normalization\n",
    "        x=self.addnorm2(x, crossAttentionOutput)\n",
    "\n",
    "        # 3. feed-forward network-> position-wise transformation\n",
    "        ffn_out=self.ffn(x)\n",
    "\n",
    "        # add & Norm-> residual connection and layer normalization\n",
    "        x=self.addnorm3(x, ffn_out)\n",
    "\n",
    "        # if visualize is True, return attention weights for analysis\n",
    "        if visualize:\n",
    "            return x, attentionWeights\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07126fb7",
   "metadata": {},
   "source": [
    "##### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, sizeOfVocab, embedDimension, numHeads, feedForwardDimension, numEncoderLayers, numDecoderLayers, maxLength=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # embedding layers for source and target sequences (with positional encoding)\n",
    "        self.src_embedding=TokenPositionalEmbedding(sizeOfVocab, embedDimension, maxLength)\n",
    "        self.target_embedding=TokenPositionalEmbedding(sizeOfVocab, embedDimension, maxLength)\n",
    "\n",
    "        # stack of encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embedDimension, numHeads, feedForwardDimension, dropout)\n",
    "            for _ in range(numEncoderLayers)\n",
    "        ])\n",
    "\n",
    "        # stack of decoder layers\n",
    "        self.decoder_layers=nn.ModuleList([\n",
    "            DecoderLayer(embedDimension, numHeads, feedForwardDimension, dropout)\n",
    "            for _ in range(numDecoderLayers)\n",
    "        ])\n",
    "\n",
    "        # final linear layer to project decoder output to vocabulary logits\n",
    "        self.output_proj=nn.Linear(embedDimension, sizeOfVocab)\n",
    "\n",
    "        # initialize parameters (weights)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # xavier uniform initialization for all weights with more than 1 dimension\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, target, src_mask=None, targetMask=None, memoryMask=None, visualize=False):\n",
    "        attentionWeights = [] if visualize else None\n",
    "\n",
    "        # 1. embed source and target tokens (add positional encoding)\n",
    "        src_emb=self.src_embedding(src)  # (batch_size,src_seq_len,embedDimension)\n",
    "        target_emb=self.target_embedding(target)  # (batch_size,target_seq_len,embedDimension)\n",
    "\n",
    "        # 2. pass source embeddings through encoder stack\n",
    "        enc_out=src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            if visualize:\n",
    "                # Get encoder output and attention weights for visualization\n",
    "                enc_out, enc_attn=layer(enc_out, mask=src_mask, visualize=True)\n",
    "                attentionWeights.append({'encoder': enc_attn})\n",
    "            else:\n",
    "                enc_out=layer(enc_out, mask=src_mask)\n",
    "\n",
    "        # 3. pass target embeddings and encoder output through decoder stack\n",
    "        dec_out=target_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            if visualize:\n",
    "                # get decoder output and attention weights for visualization\n",
    "                dec_out, decoderAttention=layer(dec_out, enc_out, targetMask=targetMask, memoryMask=memoryMask, visualize=True)\n",
    "                attentionWeights.append({'decoder': decoderAttention})\n",
    "            else:\n",
    "                dec_out=layer(dec_out, enc_out, targetMask=targetMask, memoryMask=memoryMask)\n",
    "\n",
    "        # 4. project decoder output to vocabulary logits for each position\n",
    "        logits=self.output_proj(dec_out)  # (batch_size,target_seq_len,sizeOfVocab)\n",
    "\n",
    "        # return logits and optionally attention weights for visualization\n",
    "        if visualize:\n",
    "            return logits, attentionWeights\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model hyperparameters\n",
    "sizeOfVocab = 10000\n",
    "embedDimension = 512\n",
    "numHeads = 8\n",
    "feedForwardDimension = 2048\n",
    "numEncoderLayers = 6\n",
    "numDecoderLayers = 6\n",
    "maxLength = 512\n",
    "dropout = 0.1\n",
    "\n",
    "# instantiate the Transformer model\n",
    "model = Transformer(\n",
    "    sizeOfVocab=sizeOfVocab,\n",
    "    embedDimension=embedDimension,\n",
    "    numHeads=numHeads,\n",
    "    feedForwardDimension=feedForwardDimension,\n",
    "    numEncoderLayers=numEncoderLayers,\n",
    "    numDecoderLayers=numDecoderLayers,\n",
    "    maxLength=maxLength,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# move model to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
