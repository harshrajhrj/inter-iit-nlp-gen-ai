{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311cdc57",
   "metadata": {},
   "source": [
    "##### Input Embeddings and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aede67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abb9f0",
   "metadata": {},
   "source": [
    "1. token embedding player\n",
    "2. constant positional encoding matrix (sinusoidal pattern)\n",
    "3. return token + positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa182238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_len=512):\n",
    "        super().__init__()\n",
    "        # token embedding layer: we are mapping token indices to embedding vectors\n",
    "        self.token_embedding=nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dim=embed_dim\n",
    "\n",
    "        # creating constant positional encoding matrix with sinusoidal pattern\n",
    "        pe=torch.zeros(max_len, embed_dim)\n",
    "        position=torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # shape: (max_len, 1)\n",
    "\n",
    "        # computing the div_term for the sinusoidal frequencies\n",
    "        div_term=torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        # applying sine to even indices in the embedding dimension\n",
    "        pe[:, 0::2]=torch.sin(position * div_term)\n",
    "\n",
    "        # applying cosine to odd indices in the embedding dimension\n",
    "        pe[:, 1::2]=torch.cos(position * div_term)\n",
    "\n",
    "        # register pe as a buffer so it's saved with the model but not as a parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # \"x\" is tensor of shape (batch_size, seq_len) with token indices\n",
    "        seq_len=x.size(1)\n",
    "\n",
    "        # get token embeddings: (batch_size, seq_len, embed_dim)\n",
    "        token_emb=self.token_embedding(x)\n",
    "\n",
    "        # get positional embeddings for the sequence length: (1, seq_len, embed_dim)\n",
    "        pos_emb=self.pe[:seq_len, :].unsqueeze(0)\n",
    "\n",
    "        # add token and positional embeddings\n",
    "        return token_emb + pos_emb # tensor of shape (batch_size, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ec6fd",
   "metadata": {},
   "source": [
    "##### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, visualize=False):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        query: Tensor of shape (..., seq_len_q, d_k)\n",
    "        key: Tensor of shape (..., seq_len_k, d_k)\n",
    "        value: Tensor of shape (..., seq_len_v, d_v)\n",
    "        mask: (optional) Tensor broadcastable to (..., seq_len_q, seq_len_k), with 0 for masked positions\n",
    "        visualize: (optional) If True, returns attention weights for visualization\n",
    "\n",
    "    Returns:\n",
    "        op: Attention output tensor (..., seq_len_q, d_v)\n",
    "        attn_weights: (optional) Attention weights (..., seq_len_q, seq_len_k) if visualize=True\n",
    "    \"\"\"\n",
    "    d_k=query.size(-1)  # get the dimension of the key (embedding size)\n",
    "\n",
    "    # computing raw attention scores by matrix multiplying query and key_transpose, then scale\n",
    "    scores=torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # if a mask is provided, set masked positions to a large negative value\n",
    "    if mask is not None:\n",
    "        scores=scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # applying softmax to get normalized attention weights\n",
    "    attn_weights=torch.softmax(scores, dim=-1)\n",
    "\n",
    "    # multiplying attention weights by the value vectors to get the output\n",
    "    op=torch.matmul(attn_weights, value)\n",
    "\n",
    "    # if visualize is True, return both output and attention weights\n",
    "    if visualize:\n",
    "        return op, attn_weights\n",
    "    # otherwise, return only the output\n",
    "    return op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d993f0",
   "metadata": {},
   "source": [
    "##### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.embed_dim=embed_dim\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=embed_dim // num_heads\n",
    "\n",
    "        # linear layers to project input to queries, keys, and values\n",
    "        self.q_proj=nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj=nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj=nn.Linear(embed_dim, embed_dim)\n",
    "        # final linear layer to combine outputs from all heads\n",
    "        self.out_proj=nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, visualize=False):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "\n",
    "        # auxiliary function to reshape input for multi-head attention\n",
    "        def shape(x):\n",
    "            # (batch_size, seq_len, embed_dim) -> (batch_size, num_heads, seq_len, head_dim)\n",
    "            return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Project inputs to multi-head Q, K, V\n",
    "        q=shape(self.q_proj(query))\n",
    "        k=shape(self.k_proj(key))\n",
    "        v=shape(self.v_proj(value))\n",
    "\n",
    "        # if mask is provided, expand its dimensions for all heads\n",
    "        if mask is not None:\n",
    "            mask=mask.unsqueeze(1)  # (batch_size, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "        # compute attention output (and optionally attention weights) for all heads\n",
    "        attn_output=scaled_dot_product_attention(q, k, v, mask=mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            attn_output, attn_weights = attn_output\n",
    "\n",
    "        # concatenate outputs from all heads\n",
    "        attn_output=attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # final linear projection\n",
    "        output=self.out_proj(attn_output)\n",
    "\n",
    "        # return output (and attention weights if visualize=True)\n",
    "        if visualize:\n",
    "            return output, attn_weights\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32949a5",
   "metadata": {},
   "source": [
    "##### Feed-Forward Networks and Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(embed_dim, ffn_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm=nn.LayerNorm(embed_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer_out):\n",
    "        # residual connection followed by layer normalization\n",
    "        return self.norm(x + self.dropout(sublayer_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4852338",
   "metadata": {},
   "source": [
    "##### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5cee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # multi-head self-attention mechanism\n",
    "        self.self_attn=MultiHeadAttention(embed_dim, num_heads)\n",
    "\n",
    "        # add & Norm layer after self-attention\n",
    "        self.addnorm1=AddNorm(embed_dim, dropout)\n",
    "\n",
    "        # position-wise Feed-Forward Network\n",
    "        self.ffn=PositionwiseFeedForward(embed_dim, ffn_dim, dropout)\n",
    "\n",
    "        # add & Norm layer after feed-forward\n",
    "        self.addnorm2=AddNorm(embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x, mask=None, visualize=False):\n",
    "        # self-attention sublayer with residual connection and normalization\n",
    "        # attn_out: output of self-attention; attn_weights: attention weights (if visualize=True)\n",
    "        attn_out=self.self_attn(x, x, x, mask=mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            attn_out, attn_weights = attn_out  # unpack output and attention weights\n",
    "\n",
    "        # add & Norm: residual connection (x + attn_out) followed by layer normalization\n",
    "        x=self.addnorm1(x, attn_out)\n",
    "\n",
    "        # feed-forward sublayer with residual connection and normalization\n",
    "        ffn_out=self.ffn(x)\n",
    "\n",
    "        # add & Norm: residual connection (x + ffn_out) followed by layer normalization\n",
    "        x=self.addnorm2(x, ffn_out)\n",
    "        if visualize:\n",
    "            return x, attn_weights  # return output and attention weights for visualization\n",
    "        return x  # return output only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85879898",
   "metadata": {},
   "source": [
    "##### Decoder Layer with Masked Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # masked multi-head self-attention (causal): attends to previous tokens only\n",
    "        self.self_attn=MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.addnorm1=AddNorm(embed_dim, dropout)\n",
    "\n",
    "        # encoder-decoder (cross) attention: attends to encoder outputs\n",
    "        self.cross_attn=MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.addnorm2=AddNorm(embed_dim, dropout)\n",
    "\n",
    "        # position-wise feed-forward network\n",
    "        self.ffn=PositionwiseFeedForward(embed_dim, ffn_dim, dropout)\n",
    "        self.addnorm3=AddNorm(embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, tgt_mask=None, memory_mask=None, visualize=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, tgt_seq_len, embed_dim) - decoder input\n",
    "            enc_out: (batch_size, src_seq_len, embed_dim) - encoder output\n",
    "            tgt_mask: (optional) mask for target sequence (causal mask)\n",
    "            memory_mask: (optional) mask for encoder-decoder attention\n",
    "            visualize: (optional) if True, returns attention weights\n",
    "        Returns:\n",
    "            output: (batch_size, tgt_seq_len, embed_dim)\n",
    "            attn_weights: (optional) dict of attention weights if visualize=True\n",
    "        \"\"\"\n",
    "        attn_weights={}\n",
    "\n",
    "        # we are numbering the steps for clarity\n",
    "        # 1. masked self-attention: each position can only attend to earlier positions (causal)\n",
    "        self_attn_out=self.self_attn(x, x, x, mask=tgt_mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            self_attn_out, self_attn_weights=self_attn_out  # unpack output and attention weights\n",
    "            attn_weights['self_attn']=self_attn_weights\n",
    "\n",
    "        # add & Norm: residual connection and layer normalization\n",
    "        x=self.addnorm1(x, self_attn_out)\n",
    "\n",
    "        # 2. cross-attention: decoder attends to encoder outputs (full attention)\n",
    "        cross_attn_out=self.cross_attn(x, enc_out, enc_out, mask=memory_mask, visualize=visualize)\n",
    "        if visualize:\n",
    "            cross_attn_out, cross_attn_weights=cross_attn_out  # unpack output and attention weights\n",
    "            attn_weights['cross_attn']=cross_attn_weights\n",
    "\n",
    "        # add & Norm: residual connection and layer normalization\n",
    "        x=self.addnorm2(x, cross_attn_out)\n",
    "\n",
    "        # 3. feed-forward network: position-wise transformation\n",
    "        ffn_out=self.ffn(x)\n",
    "\n",
    "        # add & Norm: residual connection and layer normalization\n",
    "        x=self.addnorm3(x, ffn_out)\n",
    "\n",
    "        # if visualize is True, return attention weights for analysis\n",
    "        if visualize:\n",
    "            return x, attn_weights\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07126fb7",
   "metadata": {},
   "source": [
    "##### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ffn_dim, num_encoder_layers, num_decoder_layers, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # embedding layers for source and target sequences (with positional encoding)\n",
    "        self.src_embedding=TokenPositionalEmbedding(vocab_size, embed_dim, max_len)\n",
    "        self.target_embedding=TokenPositionalEmbedding(vocab_size, embed_dim, max_len)\n",
    "\n",
    "        # stack of encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, ffn_dim, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # stack of decoder layers\n",
    "        self.decoder_layers=nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, ffn_dim, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # final linear layer to project decoder output to vocabulary logits\n",
    "        self.output_proj=nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # initialize parameters (weights)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # xavier uniform initialization for all weights with more than 1 dimension\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, target, src_mask=None, target_mask=None, memory_mask=None, visualize=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - source token indices\n",
    "            target: (batch_size, target_seq_len) - target token indices\n",
    "            src_mask: (optional) mask for source sequence (e.g., padding mask)\n",
    "            target_mask: (optional) mask for target sequence (e.g., causal mask)\n",
    "            memory_mask: (optional) mask for encoder-decoder attention\n",
    "            visualize: (optional) if True, returns attention weights for analysis\n",
    "        Returns:\n",
    "            logits: (batch_size, target_seq_len, vocab_size)\n",
    "            attn_weights: (optional) list of attention weights if visualize=True\n",
    "        \"\"\"\n",
    "        attn_weights = [] if visualize else None\n",
    "\n",
    "        # 1. embed source and target tokens (add positional encoding)\n",
    "        src_emb=self.src_embedding(src)  # (batch_size, src_seq_len, embed_dim)\n",
    "        target_emb=self.target_embedding(target)  # (batch_size, target_seq_len, embed_dim)\n",
    "\n",
    "        # 2. pass source embeddings through encoder stack\n",
    "        enc_out=src_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            if visualize:\n",
    "                # Get encoder output and attention weights for visualization\n",
    "                enc_out, enc_attn=layer(enc_out, mask=src_mask, visualize=True)\n",
    "                attn_weights.append({'encoder': enc_attn})\n",
    "            else:\n",
    "                enc_out=layer(enc_out, mask=src_mask)\n",
    "\n",
    "        # 3. pass target embeddings and encoder output through decoder stack\n",
    "        dec_out=target_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            if visualize:\n",
    "                # get decoder output and attention weights for visualization\n",
    "                dec_out, dec_attn=layer(dec_out, enc_out, target_mask=target_mask, memory_mask=memory_mask, visualize=True)\n",
    "                attn_weights.append({'decoder': dec_attn})\n",
    "            else:\n",
    "                dec_out=layer(dec_out, enc_out, target_mask=target_mask, memory_mask=memory_mask)\n",
    "\n",
    "        # 4. project decoder output to vocabulary logits for each position\n",
    "        logits=self.output_proj(dec_out)  # (batch_size, target_seq_len, vocab_size)\n",
    "\n",
    "        # return logits and optionally attention weights for visualization\n",
    "        if visualize:\n",
    "            return logits, attn_weights\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1a5d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "vocab_size = 10000\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "ffn_dim = 2048\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "max_len = 512\n",
    "dropout = 0.1\n",
    "\n",
    "# Instantiate the Transformer model\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ffn_dim=ffn_dim,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    max_len=max_len,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Move model to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92538196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42255185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a small real dataset from HuggingFace Datasets (e.g., \"ag_news\" for demonstration)\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load a small subset of AG News for demonstration\n",
    "dataset = load_dataset(\"ag_news\", split=\"train[:32]\")  # Only 32 samples for quick demo\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Build vocabulary from the dataset\n",
    "counter = Counter()\n",
    "for item in dataset:\n",
    "    counter.update(simple_tokenizer(item['text']))\n",
    "\n",
    "specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "itos = specials + sorted(counter)\n",
    "stoi = {tok: idx for idx, tok in enumerate(itos)}\n",
    "\n",
    "PAD_IDX = stoi['<pad>']\n",
    "BOS_IDX = stoi['<bos>']\n",
    "EOS_IDX = stoi['<eos>']\n",
    "\n",
    "def encode(text, seq_len=32):\n",
    "    tokens = [BOS_IDX] + [stoi.get(tok, stoi['<unk>']) for tok in simple_tokenizer(text)] + [EOS_IDX]\n",
    "    if len(tokens) < seq_len:\n",
    "        tokens += [PAD_IDX] * (seq_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:seq_len]\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "SEQ_LEN = 16\n",
    "\n",
    "data = [encode(item['text'], seq_len=SEQ_LEN) for item in dataset]\n",
    "data = torch.stack(data)\n",
    "train_loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    src = batch[:, :-1]\n",
    "    tgt = batch[:, :-1]\n",
    "    tgt_y = batch[:, 1:]\n",
    "    tgt_mask = torch.tril(torch.ones((SEQ_LEN-1, SEQ_LEN-1), device=device)).unsqueeze(0)\n",
    "    logits = model(src, tgt, tgt_mask=tgt_mask)\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), tgt_y.reshape(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    break  # Remove this break for full training\n",
    "\n",
    "# Inference: generate text from a prompt\n",
    "model.eval()\n",
    "prompt = \"breaking news\"\n",
    "input_ids = [BOS_IDX] + [stoi.get(tok, stoi['<unk>']) for tok in simple_tokenizer(prompt)]\n",
    "input_tensor = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "generated = input_tensor\n",
    "\n",
    "for _ in range(SEQ_LEN - len(input_ids)):\n",
    "    tgt_mask = torch.tril(torch.ones((generated.size(1), generated.size(1)), device=device)).unsqueeze(0)\n",
    "    logits = model(generated, generated, tgt_mask=tgt_mask)\n",
    "    next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "    generated = torch.cat([generated, next_token], dim=1)\n",
    "    if next_token.item() == EOS_IDX:\n",
    "        break\n",
    "\n",
    "# Decode generated tokens\n",
    "output_tokens = [itos[idx] for idx in generated[0].tolist()]\n",
    "print(\"Generated:\", \" \".join(output_tokens))\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Download and preprocess a small text dataset (e.g., Penn Treebank from torchtext)\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Load train data and build vocab\n",
    "train_iter = PennTreebank(split='train')\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 32\n",
    "PAD_IDX = vocab['<pad>']\n",
    "BOS_IDX = vocab['<bos>']\n",
    "EOS_IDX = vocab['<eos>']\n",
    "\n",
    "# Encode function\n",
    "def encode(text):\n",
    "    tokens = [BOS_IDX] + vocab(tokenizer(text)) + [EOS_IDX]\n",
    "    if len(tokens) < SEQ_LEN:\n",
    "        tokens += [PAD_IDX] * (SEQ_LEN - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:SEQ_LEN]\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# Prepare dataset\n",
    "train_iter = PennTreebank(split='train')\n",
    "data = [encode(line) for line in train_iter]\n",
    "data = torch.stack(data)\n",
    "train_loader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Training loop (1 epoch for demonstration)\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "for batch in train_loader:\n",
    "    batch = batch.to(device)\n",
    "    src = batch[:, :-1]\n",
    "    tgt = batch[:, :-1]\n",
    "    tgt_y = batch[:, 1:]\n",
    "    tgt_mask = torch.tril(torch.ones((SEQ_LEN-1, SEQ_LEN-1), device=device)).unsqueeze(0)\n",
    "    logits = model(src, tgt, tgt_mask=tgt_mask)\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), tgt_y.reshape(-1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    break  # Remove this break for full training\n",
    "\n",
    "# Inference: generate text from a prompt\n",
    "model.eval()\n",
    "prompt = \"the company\"\n",
    "input_ids = [BOS_IDX] + vocab(tokenizer(prompt))\n",
    "input_tensor = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "generated = input_tensor\n",
    "\n",
    "for _ in range(SEQ_LEN - len(input_ids)):\n",
    "    tgt_mask = torch.tril(torch.ones((generated.size(1), generated.size(1)), device=device)).unsqueeze(0)\n",
    "    logits = model(generated, generated, tgt_mask=tgt_mask)\n",
    "    next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "    generated = torch.cat([generated, next_token], dim=1)\n",
    "    if next_token.item() == EOS_IDX:\n",
    "        break\n",
    "\n",
    "# Decode generated tokens\n",
    "output_tokens = [vocab.get_itos()[idx] for idx in generated[0].tolist()]\n",
    "print(\"Generated:\", \" \".join(output_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
